{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Video Game Rating - Classification Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes into detail improving from a naive bag of words model. To see introductory data analysis and a \n",
    "regressive comparision to this task, please see attached notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import sklearn\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score as jaccard\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import scipy.optimize\n",
    "import string\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:    \n",
    "        yield json.loads(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(parse(\"./Video_Games_5.json.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    299759\n",
       "4.0     93654\n",
       "3.0     49146\n",
       "1.0     30883\n",
       "2.0     24135\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overall'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [d for d in data]\n",
    "y = [d['overall'] for d in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle data\n",
    "Xy = list(zip(X,y))\n",
    "random.shuffle(Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([d[0] for d in Xy])\n",
    "y = np.array([d[1] for d in Xy])\n",
    "y = y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['len_rev'] = df['reviewText'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall': 5.0,\n",
       " 'verified': True,\n",
       " 'reviewTime': '10 17, 2015',\n",
       " 'reviewerID': 'A1HP7NVNPFMA4N',\n",
       " 'asin': '0700026657',\n",
       " 'reviewerName': 'Ambrosia075',\n",
       " 'reviewText': \"This game is a bit hard to get the hang of, but when you do it's great.\",\n",
       " 'summary': \"but when you do it's great.\",\n",
       " 'unixReviewTime': 1445040000}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewerID\n",
       "A0059486XI1Z0P98KP35     5\n",
       "A0220159ZRNBTRKLG08H     6\n",
       "A0266076X6KPZ6CCHGVS    14\n",
       "A0277912HT4JSJKVSL3E    10\n",
       "A02836981FYG9912C66F     7\n",
       "                        ..\n",
       "AZZNK89PXD006            7\n",
       "AZZQCK9ZAKMFR           11\n",
       "AZZT1ERHBSNQ8            7\n",
       "AZZTC2OYVNE2Q            6\n",
       "AZZTOUKVTUMVM            6\n",
       "Length: 55223, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['reviewerID']).size()\n",
    "# ['overall'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>style</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [overall, verified, reviewTime, reviewerID, asin, reviewerName, reviewText, summary, unixReviewTime, vote, style, image]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['reviewerID'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497577"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497577"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = X[:50000]\n",
    "Xvalid = X[50000:60000]\n",
    "#Xtest = X[40000:50000]\n",
    "\n",
    "ytrain = y[:50000]\n",
    "yvalid = y[50000:60000]\n",
    "\n",
    "\n",
    "#ytest = y[40000:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [.01, .1, 1, 10, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigrams,  punc,  tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unigrams, keep punc, tfidf\n",
    "#training data\n",
    "unigrams = defaultdict(int)\n",
    "for d in Xtrain:\n",
    "    #not all data has a review\n",
    "    if 'reviewText' in d:\n",
    "#     token = nltk.word_tokenize(d['text'])\n",
    "#     unigram = list(ngrams(token, 1))\n",
    "        t = d['reviewText']\n",
    "        text = \" \".join(t.splitlines())\n",
    "        unigram = text.strip().split()\n",
    "        for u in unigram:\n",
    "            unigrams[u] += 1\n",
    "\n",
    "#1000 most common from training set\n",
    "mostCommonUni =sorted(unigrams.items(),key=lambda v: v[1],reverse=True)[:500]\n",
    "unigram_words = [u[0] for u in mostCommonUni]\n",
    "unigramId = dict(zip(unigram_words, range(len(unigram_words))))\n",
    "unigramSet = set(unigram_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docFreq and tf\n",
    "#training data\n",
    "docFreq = defaultdict(set)\n",
    "for d in Xtrain:\n",
    "    if 'reviewText' in d: \n",
    "        t = d['reviewText']\n",
    "        text = \" \".join(t.splitlines())\n",
    "        unigram = text.strip().split()\n",
    "        for u in unigram:\n",
    "            docFreq[u].add(d['reviewerID'])\n",
    "\n",
    "#term freq\n",
    "tf = unigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_uni_punc_tfidf(datum):\n",
    "    feat = [0]*len(unigramSet)\n",
    "    if 'reviewText' in datum: \n",
    "        t = datum['reviewText']\n",
    "        text = \" \".join(t.splitlines())\n",
    "        unigram_words = text.strip().split()\n",
    "    \n",
    "        for u in unigram_words:\n",
    "            if not (u in unigramSet): continue\n",
    "            tf_idf_word = np.log(len(Xtrain)/ len(docFreq[u])) * tf[u]\n",
    "            feat[unigramId[u]] = tf_idf_word\n",
    "\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_1 = [feature_uni_punc_tfidf(d) for d in Xtrain]\n",
    "Xvalid_1 = [feature_uni_punc_tfidf(d) for d in Xvalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigrams, No punc, tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigrams, discard punc, tfidf\n",
    "def feature_uni_nopunc_tfidf(datum):\n",
    "    feat = [0]*len(unigramSet)\n",
    "    if 'reviewText' in datum:\n",
    "        t = datum['reviewText']\n",
    "        t = ''.join([c for c in t.lower() if not c in punctuation])\n",
    "\n",
    "        text = \" \".join(t.splitlines())\n",
    "        unigram_words = text.strip().split()\n",
    "    #     token = nltk.word_tokenize(t)\n",
    "    #     unigram_words = list(ngrams(token, 1))\n",
    "\n",
    "        for u in unigram_words:\n",
    "            if not (u in unigramSet): continue\n",
    "            tf_idf_word = np.log(len(Xtrain)/ len(docFreq[u])) * tf[u]\n",
    "            feat[unigramId[u]] = tf_idf_word\n",
    "\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_2 = [feature_uni_nopunc_tfidf(d) for d in Xtrain]\n",
    "Xvalid_2 = [feature_uni_nopunc_tfidf(d) for d in Xvalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigramSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigrams, punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigrams, keep punc, counts\n",
    "def feature_uni_punc_wc(datum):\n",
    "    feat = [0]*len(unigramSet)\n",
    "    if 'reviewText' in datum: \n",
    "        t = datum['reviewText']\n",
    "    #     token = nltk.word_tokenize(t)\n",
    "    #     unigram_words = list(ngrams(token, 1))\n",
    "        text = \" \".join(t.splitlines())\n",
    "        unigram_words = text.strip().split()\n",
    "\n",
    "        for u in unigram_words:\n",
    "            if not (u in unigramSet): continue\n",
    "            feat[unigramId[u]] += 1\n",
    "\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_3 = [feature_uni_punc_wc(d) for d in Xtrain]\n",
    "Xvalid_3 = [feature_uni_punc_wc(d) for d in Xvalid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigrams, No punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigrams, discard punc, counts\n",
    "def feature_uni_nopunc_wc(datum):\n",
    "    feat = [0]*len(unigramSet)\n",
    "    if 'reviewText' in datum: \n",
    "        t = datum['reviewText']\n",
    "        t = ''.join([c for c in t.lower() if not c in punctuation])\n",
    "\n",
    "        text = \" \".join(t.splitlines())\n",
    "        unigram_words = text.strip().split()\n",
    "    #     token = nltk.word_tokenize(t)\n",
    "    #     unigram_words = list(ngrams(token, 1))\n",
    "        for u in unigram_words:\n",
    "            if not (u in unigramSet): continue\n",
    "            feat[unigramId[u]] += 1\n",
    "\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_4 = [feature_uni_nopunc_wc(d) for d in Xtrain]\n",
    "Xvalid_4 = [feature_uni_nopunc_wc(d) for d in Xvalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start of bigram models\n",
    "bigrams = defaultdict(int)\n",
    "\n",
    "for d in Xtrain:\n",
    "#     token = nltk.word_tokenize(d['text'])\n",
    "#     bigram = list(ngrams(token, 2)) \n",
    "    if 'reviewText' in d: \n",
    "        text = \" \".join(d['reviewText'].splitlines())\n",
    "        bigram = [b for b in zip(text.split(\" \")[:-1], text.split(\" \")[1:])]\n",
    "        for b in bigram:\n",
    "            bigrams[b] += 1\n",
    "        \n",
    "#1000 most common from training set\n",
    "mostCommonBi =sorted(bigrams.items(),key=lambda v: v[1],reverse=True)[:5000]\n",
    "bigram_words = [u[0] for u in mostCommonBi]\n",
    "bigramId = dict(zip(bigram_words, range(len(bigram_words))))\n",
    "bigramSet = set(bigram_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docFreq and tf\n",
    "#training data\n",
    "docFreq = defaultdict(set)\n",
    "for d in Xtrain:\n",
    "#     token = nltk.word_tokenize(d['text'])\n",
    "#     bigram = list(ngrams(token, 2)) \n",
    "    if 'reviewText' in d: \n",
    "        text = \" \".join(d['reviewText'].splitlines())\n",
    "        bigram = [b for b in zip(text.split(\" \")[:-1], text.split(\" \")[1:])]\n",
    "        for b in bigram:\n",
    "            docFreq[b].add(d['reviewerID'])\n",
    "\n",
    "#term freq\n",
    "tf = bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams,  punc, tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams, keep punc, tfidf\n",
    "def feature_bi_punc_tfidf(datum):\n",
    "    feat = [0]*len(bigramSet)\n",
    "    if 'reviewText' in datum: \n",
    "        t = datum['reviewText']\n",
    "    #     token = nltk.word_tokenize(t)\n",
    "    #     bigram_words = list(ngrams(token, 2))\n",
    "        text = \" \".join(t.splitlines())\n",
    "        bigram_words = [b for b in zip(text.split(\" \")[:-1], text.split(\" \")[1:])]\n",
    "\n",
    "        for b in bigram_words:\n",
    "            if not (b in bigramSet): continue\n",
    "            tf_idf_word = np.log(len(Xtrain)/ len(docFreq[b])) * tf[b]\n",
    "            feat[bigramId[b]] = tf_idf_word\n",
    "\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_5 = [feature_bi_punc_tfidf(d) for d in Xtrain]\n",
    "Xvalid_5 = [feature_bi_punc_tfidf(d) for d in Xvalid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams, No punc, tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams, discard punc, tfidf\n",
    "def feature_bi_nopunc_tfidf(datum):\n",
    "    feat = [0]*len(bigramSet)\n",
    "    if 'reviewText' in datum: \n",
    "        t = datum['reviewText']\n",
    "    #     token = nltk.word_tokenize(t)\n",
    "    #     bigram_words = list(ngrams(token, 2))\n",
    "        t = ''.join([c for c in t.lower() if not c in punctuation])\n",
    "        text = \" \".join(t.splitlines())\n",
    "        bigram_words = [b for b in zip(text.split(\" \")[:-1], text.split(\" \")[1:])]\n",
    "\n",
    "        for b in bigram_words:\n",
    "            if not (b in bigramSet): continue\n",
    "            tf_idf_word = np.log(len(Xtrain)/ len(docFreq[b])) * tf[b]\n",
    "            feat[bigramId[b]] = tf_idf_word\n",
    "\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_6 = [feature_bi_nopunc_tfidf(d) for d in Xtrain]\n",
    "Xvalid_6 = [feature_bi_nopunc_tfidf(d) for d in Xvalid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams, punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams, keep punc, counts\n",
    "def feature_bi_punc_wc(datum):\n",
    "    feat = [0]*len(bigramSet)\n",
    "    if 'reviewText' in datum: \n",
    "        t = datum['reviewText']\n",
    "\n",
    "    #     token = nltk.word_tokenize(t)\n",
    "    #     bigram_words = list(ngrams(token, 2))\n",
    "        text = \" \".join(t.splitlines())\n",
    "        bigram_words = [b for b in zip(text.split(\" \")[:-1], text.split(\" \")[1:])]\n",
    "\n",
    "        for b in bigram_words:\n",
    "            if not (b in bigramSet): continue\n",
    "            feat[bigramId[b]] += 1\n",
    "\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_7 = [feature_bi_punc_wc(d) for d in Xtrain]\n",
    "Xvalid_7 = [feature_bi_punc_wc(d) for d in Xvalid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams, No punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams, discard punc, counts\n",
    "def feature_bi_nopunc_wc(datum):\n",
    "    feat = [0]*len(bigramSet)\n",
    "    if 'reviewText' in datum: \n",
    "        t = datum['reviewText']\n",
    "\n",
    "    #     token = nltk.word_tokenize(t)\n",
    "    #     bigram_words = list(ngrams(token, 2))\n",
    "\n",
    "        t = ''.join([c for c in t.lower() if not c in punctuation])\n",
    "        text = \" \".join(t.splitlines())\n",
    "        bigram_words = [b for b in zip(text.split(\" \")[:-1], text.split(\" \")[1:])]\n",
    "\n",
    "        for b in bigram_words:\n",
    "            if not (b in bigramSet): continue\n",
    "            feat[bigramId[b]] += 1\n",
    "\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_8 = [feature_bi_nopunc_wc(d) for d in Xtrain]\n",
    "Xvalid_8 = [feature_bi_nopunc_wc(d) for d in Xvalid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6283\n"
     ]
    }
   ],
   "source": [
    "# pipeline\n",
    "# to_fit = [Xtrain_1, Xtrain_2, Xtrain_3, Xtrain_4, Xtrain_5, Xtrain_6, Xtrain_7, Xtrain_8]\n",
    "to_fit = [Xtrain_4]\n",
    "# to_pred = [Xvalid_1, Xvalid_2, Xvalid_3, Xvalid_4, Xvalid_5, Xvalid_6, Xvalid_7, Xvalid_8]\n",
    "to_pred = [Xvalid_4]\n",
    "model_performances = []\n",
    "for i in range(len(to_fit)):\n",
    "    #for c in C:\n",
    "        c = 0.1\n",
    "        clf = LogisticRegression(C = c, fit_intercept=False, max_iter = 10000) \n",
    "        clf.fit(to_fit[i], ytrain)\n",
    "        theta = clf.coef_\n",
    "        predictions = clf.predict(to_pred[i])\n",
    "        correct = predictions == yvalid\n",
    "        acc = sum(correct) / len(correct)\n",
    "        print(acc)\n",
    "        model_performances.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5963"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([d for d in Xvalid if d['overall'] == 5])/len(Xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6035"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([d for d in Xvalid if d['overall'] == 5])/len(Xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"unigrams, keep punc, tfidf\",\n",
    "\"unigrams, discard punc, tfidf\",\n",
    "\"unigrams, keep punc, counts\",\n",
    "\"unigrams, discard punc, counts\",\n",
    "\"bigrams, keep punc, tfidf\",\n",
    "\"bigrams, discard punc, tfidf\",\n",
    "\"bigrams, keep punc, counts\",\n",
    "\"bigrams, discard punc, counts\"]\n",
    "\n",
    "index_names = []\n",
    "for model in model_names:\n",
    "    for a in A:\n",
    "        index_names.append((model,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_tuples(index_names, names=['model','regularization param'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
